{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37d820c-95fc-4b8e-8b85-6f44c3e53a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install -qU langchain-google-community \\\n",
    "                  google-cloud-discoveryengine \\\n",
    "                  langchain_ollama \\\n",
    "                  langchain_openai \\\n",
    "                  langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "164b5762-ba1c-4c79-ae77-1cb3b58a6511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_community import VertexAIMultiTurnSearchRetriever\n",
    "from langchain_google_community import VertexAISearchRetriever\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.llms import Ollama\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "PROJECT_ID = \"gsd-ai-mx-ferneutron\"\n",
    "LOCATION = \"global\"\n",
    "SEARCH_ENGINE_ID = \"llm-demo-1\"  # Set to your search app ID\n",
    "DATA_STORE_ID = \"llms-demo-1_1728585032210\"  # Set to your data store ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf6837d2-3d77-48cc-a2c3-e54520752591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = VertexAISearchRetriever(\n",
    "    project_id=PROJECT_ID,\n",
    "    location_id=LOCATION,\n",
    "    data_store_id=DATA_STORE_ID,\n",
    "    max_documents=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34308b99-56b4-4707-b651-b06d2ad988b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='tens to hundreds of billions of parameters, which are pre\n",
      "trained on massive text data, such as PaLM [31], LLaMA\n",
      "[32], and GPT-4 [33], as summarized in Table III. Compared\n",
      "1Recently, several very promising non-transformer LLMs have been pro\n",
      "posed, such as the LLMs based on structured state space models [29], [30].\n",
      "See Section VII for more details.\n",
      "\n",
      "arXiv:2402.06196v2 [cs.CL] 20 Feb 2024' metadata={'id': '3d57d97009ba640efd886145022e0071', 'source': 'gs://demo-datastore-llms/llm-page-1.pdf1', 'previous_segments': [{'content': 'that have different starting points and velocity: statistical lan\\nguage models, neural language models, pre-trained language\\nmodels and LLMs.\\nStatistical language models (SLMs) view text as a sequence\\nof words, and estimate the probability of text as the product\\nof their word probabilities. The dominating form of SLMs\\nare Markov chain models known as the n-gram models,\\nwhich compute the probability of a word conditioned on its\\nimmediate proceeding n − 1 words. Since word probabilities\\nare estimated using word and n-gram counts collected from\\ntext corpora, the model needs to deal with data sparsity (i.e.,\\nassigning zero probabilities to unseen words or n-grams) by\\nusing smoothing, where some probability mass of the model\\nis reserved for unseen n-grams [12]. N-gram models are\\nwidely used in many NLP systems. However, these models\\nare incomplete in that they cannot fully capture the diversity\\nand variability of natural language due to data sparsity.\\nEarly neural language models (NLMs) [13], [14], [15], [16]\\ndeal with data sparsity by mapping words to low-dimensional\\ncontinuous vectors (embedding vectors) and predict the next\\nword based on the aggregation of the embedding vectors of\\nits proceeding words using neural networks. The embedding\\nvectors learned by NLMs define a hidden space where the\\nsemantic similarity between vectors can be readily computed\\nas their distance. This opens the door to computing semantic\\nsimilarity of any two inputs regardless their forms (e.g., queries\\nvs. documents in Web search [17], [18], sentences in different\\nlanguages in machine translation [19], [20]) or modalities (e.g.,\\nimage and text in image captioning [21], [22]). Early NLMs are\\ntask-specific models, in that they are trained on task-specific\\ndata and their learned hidden space is task-specific.\\nPre-trained language models (PLMs), unlike early NLMs,\\nare task-agnostic. This generality also extends to the learned\\nhidden embedding space. The training and inference of PLMs\\nfollows the pre-training and fine-tuning paradigm, where lan\\nguage models with recurrent neural networks [23] or trans\\nformers [24], [25], [26] are pre-trained on Web-scale unlabeled\\ntext corpora for general tasks such as word prediction, and then\\nfinetuned to specific tasks using small amounts of (labeled)\\ntask-specific data. Recent surveys on PLMs include [8], [27],\\n[28].\\nLarge language models (LLMs) mainly refer to\\ntransformer-based neural language models\\n\\n1\\nthat contain', 'pageNumber': '1'}], 'next_segments': []}\n",
      "CPU times: user 5.99 ms, sys: 0 ns, total: 5.99 ms\n",
      "Wall time: 128 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"what about llms\"\n",
    "\n",
    "result = retriever.invoke(query)\n",
    "for doc in result:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47cbdbe9-2691-48b3-b25c-eb4f816c625c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"Use the following context to answer the question at the end. \n",
    "                If you don't know the answer, just say that you don't know, don't try   \n",
    "                to make up an answer.\\n\\nContext: {context}\\n\\nQuestion: {question}\"\"\"\n",
    ")\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.1\")\n",
    "# model = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "   # | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae1828a8-63dd-4a10-879a-4f0a7edc3437",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 200 ms, sys: 47.7 ms, total: 248 ms\n",
      "Wall time: 52.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = chain.invoke(\"what abou llms?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffe45e46-d46f-4a8d-91af-5b8fc122a1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The context discusses Large Language Models (LLMs) extensively, but I'll try to summarize the key points related to them:\\n\\n* LLMs are a type of model that has drawn significant attention due to their strong performance on natural language tasks.\\n* They have been pre-trained on massive amounts of text data, which enables general-purpose language understanding and generation.\\n* The research area of LLMs is evolving rapidly and is discussed in the paper as one of the four waves of language modeling.\\n* Three popular LLM families (GPT, LLaMA, PaLM) are reviewed in the paper, along with their characteristics, contributions, and limitations.\\n* LLMs have become a basic building block for developing general-purpose AI agents or artificial general intelligence (AGI).\\n* The field of LLMs is moving fast, with new findings, models, and techniques being published rapidly.\\n\\nHowever, I don't see an explicit mention of what happens to LLMs in the provided context. Could you please provide more context or clarify what you're looking for?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2cd48c11-d01d-4cdc-a738-5804eff60045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here is a summary of LLMS (Large Language Models):\n",
      "\n",
      "LLMs are pre-trained statistical language models based on neural networks. They have been successful due to decades of research and development in language modeling, which can be categorized into four waves:\n",
      "\n",
      "1. Statistical Language Models (SLMs)\n",
      "2. Early Neural Language Models (NLMs)\n",
      "3. Pre-Trained Language Models (PLMs)\n",
      "4. Large Language Models (LLMs)\n",
      "\n",
      "LLMs are transformer-based models pre-trained on Web-scale text corpora and have become the basic building block for general-purpose AI agents or artificial general intelligence (AGI). They can be used as task solvers, power systems like Microsoft's Co-Pilot, and perform multi-step reasoning.\n",
      "\n",
      "Key characteristics of LLMs include:\n",
      "\n",
      "* Pre-training on large amounts of text data\n",
      "* Ability to handle natural language understanding and generation tasks\n",
      "* Capacity for general-purpose language understanding and generation\n",
      "* Use of neural networks and transformer-based architectures\n",
      "\n",
      "LLMs are considered a recent advancement in the field, with new findings, models, and techniques being published rapidly. This paper aims to provide a timely survey of recent advances on LLMs, serving as a valuable resource for students, researchers, and developers.\n"
     ]
    }
   ],
   "source": [
    "x = chain.invoke(\"what is the summary of llms?\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0309648-9ea8-4f09-a39f-0502bca6dc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_4255/408293162.py:18: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.1\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
      "/var/tmp/ipykernel_4255/408293162.py:18: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  llm = Ollama(model=\"llama3.1\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "llm = Ollama(model=\"llama3.1\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e137f4ed-6248-4c03-ba7a-427e171b3c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recently, several non-transformer LLMs have been proposed, such as those based on structured state space models. These alternative approaches aim to improve upon traditional transformer-based LLMs like PaLM and GPT-4.CPU times: user 180 ms, sys: 19.4 ms, total: 199 ms\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = qa_chain({\"query\": \"what about llms?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05e29c-1e09-4a4f-bf58-e9a93b80005b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvYSz5hyT43F"
      },
      "source": [
        "Documentation: https://python.langchain.com/docs/integrations/retrievers/google_vertex_ai_search/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ls7KUBahF3zc"
      },
      "outputs": [],
      "source": [
        "#@title 1. Install requirements\n",
        "%%capture\n",
        "! pip install -qU langchain-google-community \\\n",
        "                  google-cloud-discoveryengine \\\n",
        "                  langchain_ollama\n",
        "\n",
        "\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h1zzFNlKf-Rs"
      },
      "outputs": [],
      "source": [
        "#@title 2. Authentication\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XTzWFlFmZgeQ"
      },
      "outputs": [],
      "source": [
        "#@title 3. Settings\n",
        "\n",
        "from langchain_google_community import (\n",
        "    VertexAIMultiTurnSearchRetriever,\n",
        "    VertexAISearchRetriever,\n",
        ")\n",
        "\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "PROJECT_ID = \"ferneutron-demos\"\n",
        "LOCATION = \"global\"\n",
        "SEARCH_ENGINE_ID = \"demo-llama\"  # Set to your search app ID\n",
        "DATA_STORE_ID = \"demo-llama-datastore_1729258357198\"  # Set to your data store ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y6gn2yE7ZzAh"
      },
      "outputs": [],
      "source": [
        "#@title 4. Init Retriever\n",
        "retriever = VertexAISearchRetriever(\n",
        "    project_id=PROJECT_ID,\n",
        "    location_id=LOCATION,\n",
        "    data_store_id=DATA_STORE_ID,\n",
        "    max_documents=3,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VdB4VIsEZ5Us",
        "outputId": "06ca4087-8428-4c58-eea7-12628bfe2240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='tens to hundreds of billions of parameters, which are pre\n",
            "trained on massive text data, such as PaLM [31], LLaMA\n",
            "[32], and GPT-4 [33], as summarized in Table III. Compared\n",
            "1Recently, several very promising non-transformer LLMs have been pro\n",
            "posed, such as the LLMs based on structured state space models [29], [30].\n",
            "See Section VII for more details.\n",
            "\n",
            "arXiv:2402.06196v2 [cs.CL] 20 Feb 2024' metadata={'id': 'fe2cedd49df80cd0c6ed6652ec0cbfaa', 'source': 'gs://demo-llama/llm-page-1.pdf1', 'previous_segments': [{'content': 'that have different starting points and velocity: statistical lan\\nguage models, neural language models, pre-trained language\\nmodels and LLMs.\\nStatistical language models (SLMs) view text as a sequence\\nof words, and estimate the probability of text as the product\\nof their word probabilities. The dominating form of SLMs\\nare Markov chain models known as the n-gram models,\\nwhich compute the probability of a word conditioned on its\\nimmediate proceeding n âˆ’ 1 words. Since word probabilities\\nare estimated using word and n-gram counts collected from\\ntext corpora, the model needs to deal with data sparsity (i.e.,\\nassigning zero probabilities to unseen words or n-grams) by\\nusing smoothing, where some probability mass of the model\\nis reserved for unseen n-grams [12]. N-gram models are\\nwidely used in many NLP systems. However, these models\\nare incomplete in that they cannot fully capture the diversity\\nand variability of natural language due to data sparsity.\\nEarly neural language models (NLMs) [13], [14], [15], [16]\\ndeal with data sparsity by mapping words to low-dimensional\\ncontinuous vectors (embedding vectors) and predict the next\\nword based on the aggregation of the embedding vectors of\\nits proceeding words using neural networks. The embedding\\nvectors learned by NLMs define a hidden space where the\\nsemantic similarity between vectors can be readily computed\\nas their distance. This opens the door to computing semantic\\nsimilarity of any two inputs regardless their forms (e.g., queries\\nvs. documents in Web search [17], [18], sentences in different\\nlanguages in machine translation [19], [20]) or modalities (e.g.,\\nimage and text in image captioning [21], [22]). Early NLMs are\\ntask-specific models, in that they are trained on task-specific\\ndata and their learned hidden space is task-specific.\\nPre-trained language models (PLMs), unlike early NLMs,\\nare task-agnostic. This generality also extends to the learned\\nhidden embedding space. The training and inference of PLMs\\nfollows the pre-training and fine-tuning paradigm, where lan\\nguage models with recurrent neural networks [23] or trans\\nformers [24], [25], [26] are pre-trained on Web-scale unlabeled\\ntext corpora for general tasks such as word prediction, and then\\nfinetuned to specific tasks using small amounts of (labeled)\\ntask-specific data. Recent surveys on PLMs include [8], [27],\\n[28].\\nLarge language models (LLMs) mainly refer to\\ntransformer-based neural language models\\n\\n1\\nthat contain', 'pageNumber': '1'}], 'next_segments': []}\n"
          ]
        }
      ],
      "source": [
        "#@title 5. Search\n",
        "query = \"what about llms\"\n",
        "\n",
        "result = retriever.invoke(query)\n",
        "for doc in result:\n",
        "    print(doc)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HmruFh5m73Ca"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}